{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import numpy\n",
    "\n",
    "from EngineFiles.MachineLearning import LogisticRegressionModel as LRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/indonesia_Tweet/clean_tweets.csv')\n",
    "df.dropna(subset=['Tweet'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = LRM.splitSet(df, 0.2, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "type(freqs) = <class 'dict'>\nlen(freqs) = 16279\n"
    }
   ],
   "source": [
    "freq = LRM.dictFrequency(x_train, y_train)\n",
    "print(\"type(freqs) = \" + str(type(freq)))\n",
    "print(\"len(freqs) = \" + str(len(freq.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `sigmoid(z)` function is defined as: \n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `gradientDescent(x, y, theta, alpha, num_iters)` function:\n",
    "\n",
    "* The number of iterations `num_iters` is the number of times that we'll use the entire training set.\n",
    "* For each iteration, we'll calculate the cost function using all training examples (there are `m` training examples), and for all features.\n",
    "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
    "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `extractFeatures(text, freqs)` function:\n",
    "\n",
    "* Given a list of tweets, extract the features and store them in a matrix. we will extract two features.\n",
    "    * The first feature is the number of positive words in a tweet.\n",
    "    * The second feature is the number of negative words in a tweet. \n",
    "* Then train the logistic regression classifier on these features.\n",
    "* Test the classifier on a validation set.\n",
    "* This function takes in a single tweet.\n",
    "* Process the tweet using the imported `process_tweet()` function and save the list of tweet words.\n",
    "* Loop through each word in the list of processed words\n",
    "    * For each word, check the `freqs` dictionary for the count when that word has a positive '1' label.\n",
    "    * Do the same for the count for when the word is associated with the negative label '0'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The cost after training is 0.69314718.\nThe resulting vector of weights is [-0.0, 4e-08, -9e-08]\n"
    }
   ],
   "source": [
    "X = numpy.zeros((len(x_train), 3))\n",
    "for i in range(len(x_train)):\n",
    "    X[i, :] = LRM.extractFeatures(x_train[i], freq)\n",
    "\n",
    "Y = y_train\n",
    "\n",
    "J, theta = LRM.gradientDescent(X, Y, numpy.zeros((3, 1)), 1e-9, 10000)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in numpy.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `LR_predictTweet(tweet, freqs, theta)` function\n",
    "\n",
    "Predict whether a tweet is positive or negative.\n",
    "\n",
    "* Given a tweet, process it, then extract the features.\n",
    "* Apply the model's learned weights on the features to get the logits.\n",
    "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sih sayang banget nya goeun hati banget ikhlas ajar yuri banget main vokal menang my heart goeun -> 0.499963\nagus budek dengar -> 0.499999\n"
    }
   ],
   "source": [
    "for tweet in [x_test[10],x_test[20]]:\n",
    "    print( '%s -> %f' % (tweet, LRM.LR_predictTweet(tweet, freq, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check performance using the test set\n",
    "After training the model using the training set above, check how the model might perform on real, unseen data, by testing it against the test set.\n",
    "\n",
    "### Implement `logisticRegressionAccuracy(x, y, freqs, theta)` function\n",
    "* Given the test data and the weights of the trained model, calculate the accuracy of the logistic regression model. \n",
    "* Use `LR_predictTweet()` function to make predictions on each tweet in the test set.\n",
    "* If the prediction is > 0.5, set the model's classification `y_pred` to 1, otherwise set the model's classification `y_pred` to 0.\n",
    "* A prediction is accurate when `y_pred` equals `y_test`.  Sum up all the instances when they are equal and divide by `m`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Logistic regression model's accuracy = 0.5985\n"
    }
   ],
   "source": [
    "LR_accuracy = LRM.logisticRegressionAccuracy(x_test, y_test, freq, theta)\n",
    "print(f\"Logistic regression model's accuracy = {LR_accuracy:.4f}\")"
   ]
  }
 ]
}