{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from EngineFiles.MachineLearning import NaiveBayesModel as NBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/indonesia_Tweet/clean_tweets.csv')\n",
    "df.dropna(subset=['Tweet'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = NBM.splitSet(df, 0.2, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = NBM.countTweets(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the logprior\n",
    "- the logprior is $log(D_{pos}) - log(D_{neg})$\n",
    "\n",
    "##### Calculate log likelihood\n",
    "- We can iterate over each word in the vocabulary, the `naiveBayesTrain()` function uses the `lookup()` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
    "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word.\n",
    "\n",
    "- We can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-0.15142484267089884 12327\n"
    }
   ],
   "source": [
    "logprior, loglikelihood = NBM.naiveBayesTrain(freqs, x_train, y_train)\n",
    "print(logprior, len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuration test Naive Bayes Model\n",
    "\n",
    "Now that we have the logprior and loglikelihood, we can test the naive bayes function by making predicting on some tweets\n",
    "\n",
    "The function takes in the tweet, logprior, loglikelihood.\n",
    "It returns the probability that the tweet belongs to the positive or negative class.\n",
    "For each tweet, sum up loglikelihoods of each word in the tweet.\n",
    "Also we add the logprior to this sum to get the predicted sentiment of that tweet.\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
    "\n",
    "The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood. However, whenever the data is not perfectly balanced, the logprior will be a non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Naive Bayes accuracy = 0.8337\n"
    }
   ],
   "source": [
    "acc, y_pred = NBM.naiveBayesAccuracy(x_test, y_test, logprior,loglikelihood)\n",
    "print(\"Naive Bayes accuracy = %0.4f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.8075040783034257\n0.8337442761535752\n              precision    recall  f1-score   support\n\n           0       0.81      0.90      0.85      1527\n           1       0.87      0.75      0.81      1312\n\n    accuracy                           0.83      2839\n   macro avg       0.84      0.83      0.83      2839\nweighted avg       0.84      0.83      0.83      2839\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ]
}